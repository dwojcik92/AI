{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from dataclasses import dataclass\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SDPMHA(nn.Module):\n",
    "    \"\"\"\n",
    "    Scaled Dot Product Multihead Attention\n",
    "    Implementation from page 5 from: https://arxiv.org/pdf/1706.03762\n",
    "    \"\"\"\n",
    "    def __init__(self, n_embed, seq_len, n_head, flash=True):\n",
    "        super().__init__()\n",
    "        assert n_embed%n_head == 0, \"n_embed must be divisible by n_head\"\n",
    "        self.n_head = n_head\n",
    "        self.q_projection = nn.Linear(n_embed, n_embed)\n",
    "        self.k_projection = nn.Linear(n_embed, n_embed)\n",
    "        self.v_projection = nn.Linear(n_embed, n_embed)\n",
    "        self.out_projection  = nn.Linear(n_embed, n_embed)\n",
    "        self.flash = flash\n",
    "\n",
    "        # this will be used as mask in attention:\n",
    "        # \"We need to prevent leftward information flow in the decoder to preserve \n",
    "        # the auto-regressive property. We implement this inside of scaled \n",
    "        # dot-product attention by masking out (setting to −∞) all values \n",
    "        # in the input of the softmax which correspond to illegal connections.\" (from paper)\n",
    "        # Mathematicaly this means: set upper triangle matrix to -inf. values.\n",
    "        self.register_buffer(\"bias\",torch.tril(torch.ones(seq_len,\n",
    "                                                          seq_len)).view(1,1,seq_len, seq_len))\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len, emb_dim = x.size()\n",
    "        # below you will see that we use transpose so that the final shape will be:\n",
    "        # (batch_size, sel.n_head, seq_len, emb_dim//self.n_head)\n",
    "        # from now on: emb_dim//self.n_head = head_size\n",
    "        # (batch_size, sel.n_head, seq_len, head_size)\n",
    "        Q = self.q_projection(x)\n",
    "        K = self.k_projection(x)\n",
    "        V = self.v_projection(x)\n",
    "        \n",
    "        Q = Q.view(batch_size, seq_len, self.n_head, emb_dim//self.n_head).transpose(1,2) \n",
    "        K = K.view(batch_size, seq_len, self.n_head, emb_dim//self.n_head).transpose(1,2) \n",
    "        V = V.view(batch_size, seq_len, self.n_head, emb_dim//self.n_head).transpose(1,2) \n",
    "\n",
    "\n",
    "        if self.flash:\n",
    "            # efficient attention using Flash Attention CUDA kernels\n",
    "            y = torch.nn.functional.scaled_dot_product_attention(Q, K, V, attn_mask=None, dropout_p=0, is_causal=True)\n",
    "        else:\n",
    "            # below transpose is needed to transpose along last two dimensions only\n",
    "            attention = Q@K.transpose(-2, -1) * (1.0 / math.sqrt(K.size(-1)))\n",
    "            if mask:\n",
    "                attention = attention.masked_fill(self.bias[:,:,:seq_len,:seq_len]==0, float('-inf'))\n",
    "            # softmax along last dimension (emb_dim)\n",
    "            attention = F.softmax(attention, dim=-1)\n",
    "    \n",
    "            # Just as in Karpathy code\n",
    "            # (batch_size, n_head, seq_len, seq_len) x (batch_size, n_head, seq_len, head_size)\n",
    "            # results in (batch_size, n_head, seq_len, head_size)\n",
    "            y = attention@V\n",
    "        # concat of all heads to recover final dimensions\n",
    "        # (batch_size, n_head, seq_len, head_size) -> (batch_size, seq_len, n_head*head_size)\n",
    "        # first transpose to (batch_size, seq_len, n_head, head_size)\n",
    "        # then reshape to x shape\n",
    "        # contiguous forces to have correct format in computer memory\n",
    "        y = y.transpose(1,2).contiguous().view(batch_size, seq_len, emb_dim)\n",
    "        y = self.out_projection(y)\n",
    "        # dont return attention here, as it is multiheaded and not easy to understand and use\n",
    "        # later in code\n",
    "        return y\n",
    "\n",
    "class Position_Wise_Feed_Forward(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of eq 2 from: https://arxiv.org/pdf/1706.03762\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 d_model: int = 512, \n",
    "                 activation: nn.Module = nn.GELU(approximate='tanh')):\n",
    "        \"\"\"\n",
    "        d_model = model dimensionality\n",
    "        d_ff = inner-layer dimensioality\n",
    "        activation = activation function, OpenAI uses GeLU with tanh aproximation\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        d_ff = 4 * d_model\n",
    "        self.inner = nn.Linear(d_model, d_ff)\n",
    "        self.outer = nn.Linear(d_ff, d_model)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.outer(self.activation(self.inner(x)))\n",
    "        return y\n",
    "\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    # implementation from Figure 1 \n",
    "    # from Attention is all you need: https://arxiv.org/pdf/1706.03762\n",
    "    # this is not the implementation of Karpathy\n",
    "    # and not the way OpenAI did it.\n",
    "    def __init__(self, seq_len, emb_dim, n_head):\n",
    "        super().__init__()\n",
    "        self.mha1 = SDPMHA(emb_dim, seq_len, n_head)\n",
    "        self.ln1 = nn.LayerNorm(emb_dim)\n",
    "        self.mha2 = SDPMHA(emb_dim, seq_len, n_head)\n",
    "        self.ln2 = nn.LayerNorm(emb_dim)\n",
    "        self.ff = Position_Wise_Feed_Forward(d_model=emb_dim)\n",
    "        self.ln3 = nn.LayerNorm(emb_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.ln1(x+self.mha1(x, mask=False))\n",
    "        x = self.ln2(x+self.mha2(x, mask=False))\n",
    "        x = self.ln3(self.ff(x))\n",
    "        return x\n",
    "\n",
    "class DecoderBlockOpenAI(nn.Module):\n",
    "    # Implementation smillar to Karpathy\n",
    "    # and not the full way the OpenAI did it.\n",
    "    def __init__(self, seq_len, emb_dim, n_head, activation=nn.GELU(approximate='tanh')):\n",
    "        super().__init__()\n",
    "        self.mha1 = SDPMHA(emb_dim, seq_len, n_head)\n",
    "        self.ln1 = nn.LayerNorm(emb_dim)\n",
    "        self.ff = Position_Wise_Feed_Forward(d_model=emb_dim, activation=activation)\n",
    "        self.ln2 = nn.LayerNorm(emb_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.ln1(x+self.mha1(x, mask=True))\n",
    "        x = self.ln2(self.ff(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    context_size: int = 1024\n",
    "    vocab_size: int = 50304 # Just like Karpathy I pad it to nearest multiple of 64\n",
    "    n_layers: int = 12\n",
    "    n_head: int = 12\n",
    "    n_embeding: int = 768\n",
    "    \n",
    "\n",
    "class GPT2(nn.Module):\n",
    "    def __init__(self, config: GPTConfig) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.config = config\n",
    "        \n",
    "        # All we need to do  is to specify a single valabre with whoel transformer\n",
    "        # with a flexbile size\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            word_embeding = nn.Embedding(config.vocab_size, config.n_embeding),\n",
    "            positional_embeding = nn.Embedding(config.context_size, config.n_embeding),\n",
    "            blocks = nn.ModuleList(\n",
    "                [DecoderBlockOpenAI(seq_len=config.context_size, \n",
    "                                    emb_dim=config.n_embeding,\n",
    "                                    n_head=config.n_head) for _ in range(config.n_layers)]\n",
    "            ),\n",
    "            layer_norm = nn.LayerNorm(config.n_embeding),\n",
    "        ))\n",
    "        # the full body of transformer is completed now we need a head\n",
    "        self.lm_head = nn.Linear(config.n_embeding, config.vocab_size, bias=False)\n",
    "        \n",
    "        # Just like Karpathy said wrote in his repository the idea is to set\n",
    "        # the weights of word_embeding and lm_head to be the same\n",
    "        # source: https://paperswithcode.com/method/weight-tying\n",
    "        self.transformer.word_embeding.weight = self.lm_head.weight\n",
    "        \n",
    "        # The most important step in all machine learning\n",
    "        # initialize all weights correctly.\n",
    "        self.apply(self._init_weights)\n",
    "        #  Additionaly follow GPT-2 paper\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('out_projection.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layers))\n",
    "                \n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            \n",
    "            \n",
    "    def forward(self, tokens, targets=None):\n",
    "        device = tokens.device\n",
    "        batch_size, tokens_size = tokens.size()\n",
    "        assert tokens_size <= self.config.context_size, f\"Input tokens size: {tokens_size} is larger then context size {self.config.context_size}!\"\n",
    "        tokens_position = torch.arange(0, tokens_size, dtype=torch.long, device=device)\n",
    "        \n",
    "        # forward the input throght model\n",
    "        token_embbeding = self.transformer.word_embeding(tokens)\n",
    "        position_embeding = self.transformer.positional_embeding(tokens_position)\n",
    "        x = token_embbeding + position_embeding\n",
    "        for block in self.transformer.blocks:\n",
    "            x = block(x)\n",
    "        x = self.transformer.layer_norm(x)\n",
    "        \n",
    "        # tricks from Karpathy source code\n",
    "        if targets is not None:\n",
    "            # calcualte the loss\n",
    "            logits = self.lm_head(x)\n",
    "            loss = F.cross_entropy(logits.view(1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "        else:\n",
    "            # inference-time mini-optimization: only forward the lm_head on the very last position\n",
    "            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n",
    "            loss = None\n",
    "\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @dataclass\n",
    "# class GPTConfig:\n",
    "#     context_size: int = 1024\n",
    "#     vocab_size: int = 50304 # Just like Karpathy I pad it to nearest multiple of 64\n",
    "#     n_layers: int = 12\n",
    "#     n_head: int = 12\n",
    "#     n_embeding: int = 768\n",
    "    \n",
    "\n",
    "gpt2_standard_config = dict(n_layers = 12, n_head=12, n_embeding=768)\n",
    "gpt2_standard_config['vocab_size'] = 50304 # always 50257 for GPT model checkpoints\n",
    "gpt2_standard_config['context_size'] = 1024 # always 1024 for GPT model checkpoints\n",
    "model  = GPT2(config=GPTConfig(**gpt2_standard_config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = torch.randint(0, 50304, (1, 1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 50304])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward(test_input)[0].size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /Users/dwojcik/.cache/huggingface/modules/datasets_modules/datasets/Skylion007--openwebtext/6f68e85c16ccc770c0dd489f4008852ea9633604995addd0cd76e293aed9e521 (last modified on Sat Nov 16 10:37:31 2024) since it couldn't be found locally at Skylion007/openwebtext, or remotely on the Hugging Face Hub.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fffac60059f4fe293b64a77f82aaaf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"Skylion007/openwebtext\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretrain tokenizer\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = ByteLevelBPETokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text data from the dataset\n",
    "sample_size = 100_000\n",
    "text_column_name = 'text'  # Replace with the name of your text column\n",
    "texts = dataset['train'][:sample_size][text_column_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50304\n",
    "tokenizer.train_from_iterator(texts, \n",
    "                              vocab_size=vocab_size, \n",
    "                              min_frequency=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tokenizer/vocab.json', 'tokenizer/merges.txt']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_model(\"tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train GPT2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import GPT2Config, GPT2Model, GPT2TokenizerFast, Trainer, GPT2LMHeadModel\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import load_dataset\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /Users/dwojcik/.cache/huggingface/modules/datasets_modules/datasets/Skylion007--openwebtext/6f68e85c16ccc770c0dd489f4008852ea9633604995addd0cd76e293aed9e521 (last modified on Sat Nov 16 10:37:31 2024) since it couldn't be found locally at Skylion007/openwebtext, or remotely on the Hugging Face Hub.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93e1e219e9b94e5690dc6ba526e995b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 67.2M parameters\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d189f59fe4404758a31005b9d17e5d87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125216 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import GPT2Config, GPT2Model, GPT2TokenizerFast, Trainer, GPT2LMHeadModel\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import load_dataset\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "import gc\n",
    "\n",
    "vocab_size = 50304\n",
    "gpt2_config = GPT2Config(vocab_size=vocab_size,n_layer=4, n_head=4, n_positions=256)\n",
    "model = GPT2LMHeadModel(config=gpt2_config)\n",
    "tokenizer = GPT2TokenizerFast(vocab_file='tokenizer/vocab.json', merges_file='tokenizer/merges.txt')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize(element):\n",
    "    outputs = tokenizer(\n",
    "        element[0][\"text\"],\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=gpt2_config.n_positions,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    # Pad the sequences to length n_positions\n",
    "    input_ids_padded = torch.nn.functional.pad(outputs['input_ids'], (0, gpt2_config.n_positions - len(outputs['input_ids'])))\n",
    "    attention_mask = torch.ones_like(input_ids_padded)\n",
    "    return {\"input_ids\": input_ids_padded, \"attention_mask\": attention_mask}\n",
    "\n",
    "dataset = load_dataset(\"Skylion007/openwebtext\")\n",
    "\n",
    "dataloader = DataLoader(dataset=dataset['train'], \n",
    "                        collate_fn=tokenize, \n",
    "                        batch_size=64, \n",
    "                        pin_memory=False, \n",
    "                        )\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Initialize the model, tokenizer, data loader and the args here as shown in the code above.\n",
    "model.train() # Set the model to training mode.\n",
    "model = model.to(device)\n",
    "model_size = sum(t.numel() for t in model.parameters())\n",
    "print(f\"Model size: {model_size/1000**2:.1f}M parameters\")\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5.e-4) # Define an optimizer.\n",
    "# Create a learning rate scheduler that starts with a low learning rate and increases it over time.\n",
    "lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=100,\n",
    "    num_training_steps=len(dataset['train'])\n",
    ")\n",
    "\n",
    "\n",
    "loss_values = []\n",
    "steps = []\n",
    "\n",
    "\n",
    "\n",
    "global_step = 0\n",
    "loop = tqdm(dataloader, leave=True)\n",
    "for batch in loop:\n",
    "    input_ids = batch[\"input_ids\"].to(device) # Move the data to the correct device (CPU/GPU/MPS).\n",
    "    attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "    outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=input_ids) # Forward pass.\n",
    "    \n",
    "    loss = outputs.loss\n",
    "    \n",
    "    loss.backward() # Backward pass.\n",
    "\n",
    "    optimizer.step() # Update the parameters.\n",
    "    lr_scheduler.step()\n",
    "    \n",
    "    # Manually delete input_ids and attention_mask to free up memory\n",
    "    del input_ids\n",
    "    del attention_mask  \n",
    "    gc.collect()\n",
    "    torch.mps.empty_cache()\n",
    "    \n",
    "    optimizer.zero_grad() # Reset gradients to zero for the next iteration.\n",
    "    \n",
    "    loop.set_description_str(f\"Loss: {loss.item():.5f}, lr = {lr_scheduler.get_last_lr()[0]:.2e}\")  \n",
    "\n",
    "    loss_values.append(loss.item())\n",
    "    steps.append(global_step)\n",
    "    global_step += 1\n",
    "    \n",
    "    if global_step > 500:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, there was a the the the the the the the the the the the the the the the the the the the the the the the the the the the U, the the the the the the the the the the the the the\n"
     ]
    }
   ],
   "source": [
    "def generate_text(prompt, max_length=50):\n",
    "    # Tokenize the input prompt\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(\"mps\")  # Assuming MPS is available; change to \"cuda\" or \"cpu\" if needed\n",
    "    \n",
    "    # Generate text\n",
    "    outputs = model.generate(inputs, max_length=max_length, num_return_sequences=1)\n",
    "    \n",
    "    # Decode the generated text\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return generated_text\n",
    "\n",
    "# Example usage\n",
    "prompt = \"Once upon a time, there was a\"\n",
    "completed_text = generate_text(prompt)\n",
    "print(completed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "enc = tiktoken.get_encoding(\"o200k_base\")\n",
    "assert enc.decode(enc.encode(\"hello world\")) == \"hello world\"\n",
    "\n",
    "# To get the tokeniser corresponding to a specific model in the OpenAI API:\n",
    "enc = tiktoken.encoding_for_model(\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = tiktoken.get_encoding(\"p50k_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50281"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.n_vocab"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
