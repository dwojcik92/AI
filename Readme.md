# AI: Implementing Models from Research Papers

Welcome to the **AI** repository! This repository is dedicated to implementing machine learning models from cutting-edge research papers across various domains of Artificial Intelligence. Whether you're looking to replicate the latest breakthroughs or understand how to bring theory into practice, this repo will provide clean, well-documented implementations to help you get started.

---

## Table of Contents

1. [About](#about)
2. [Implemented Papers and Models](#implemented-papers-and-models)
3. [Installation](#installation)
4. [Usage](#usage)
6. [License](#license)

---

## About

This repository features implementations of important machine learning models, accompanied by their original research papers. Each project contains:
- Model implementations with clean, well-documented code
- Links to research papers for reference
- Instructions for running, training, and testing models
- Key insights and discussions about the model architectures

The goal is to provide a comprehensive learning resource for those who want to explore the underlying techniques of modern AI research.


## Implemented Papers and Models

Here’s the ordered list of models that will be implemented, each based on influential papers. This list spans from foundational models to cutting-edge architectures.

### 1. **Perceptron & Multilayer Perceptron (MLP)**
   - **Paper**: *The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain* (1958) by Frank Rosenblatt  
   - **Link**: [Paper](https://www.sciencedirect.com/science/article/pii/S0893608008001905)

### 2. **LeNet-5 (Convolutional Neural Networks)**
   - **Paper**: *Gradient-Based Learning Applied to Document Recognition* (1998) by Yann LeCun et al.  
   - **Link**: [Paper](http://yann.lecun.com/exdb/lenet/)

### 3. **AlexNet (Deep CNNs)**
   - **Paper**: *ImageNet Classification with Deep Convolutional Neural Networks* (2012) by Alex Krizhevsky et al.  
   - **Link**: [Paper](https://papers.nips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html)

### 4. **LSTM (Long Short-Term Memory)**
   - **Paper**: *Long Short-Term Memory* (1997) by Sepp Hochreiter and Jürgen Schmidhuber  
   - **Link**: [Paper](https://www.bioinf.jku.at/publications/older/2604.pdf)

### 5. **Attention Mechanism & Seq2Seq**
   - **Paper**: *Neural Machine Translation by Jointly Learning to Align and Translate* (2014) by Dzmitry Bahdanau et al.  
   - **Link**: [Paper](https://arxiv.org/abs/1409.0473)

### 6. **Transformer (Attention Is All You Need)**
   - **Paper**: *Attention Is All You Need* (2017) by Vaswani et al.  
   - **Link**: [Paper](https://arxiv.org/abs/1706.03762)

### 7. **Autoencoders (AE)**
   - **Paper**: *Reducing the Dimensionality of Data with Neural Networks* (2006) by Geoffrey Hinton and Ruslan Salakhutdinov  
   - **Link**: [Paper](https://www.science.org/doi/10.1126/science.1127647)

### 8. **YOLO (You Only Look Once)**
   - **Paper**: *You Only Look Once: Unified, Real-Time Object Detection* (2016) by Joseph Redmon et al.  
   - **Link**: [Paper](https://arxiv.org/abs/1506.02640)

### 9. **Variational Autoencoders (VAE)**
   - **Paper**: *Auto-Encoding Variational Bayes* (2013) by Kingma and Welling  
   - **Link**: [Paper](https://arxiv.org/abs/1312.6114)

### 10. **GAN (Generative Adversarial Networks)**
   - **Paper**: *Generative Adversarial Nets* (2014) by Ian Goodfellow et al.  
   - **Link**: [Paper](https://arxiv.org/abs/1406.2661)

### 11. **BERT (Bidirectional Encoder Representations from Transformers)**
   - **Paper**: *BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding* (2019) by Devlin et al.  
   - **Link**: [Paper](https://arxiv.org/abs/1810.04805)

### 12. **Deep Q-Network (DQN)**
   - **Paper**: *Playing Atari with Deep Reinforcement Learning* (2013) by Mnih et al.  
   - **Link**: [Paper](https://arxiv.org/abs/1312.5602)

### 13. **Proximal Policy Optimization (PPO)**
   - **Paper**: *Proximal Policy Optimization Algorithms* (2017) by Schulman et al.  
   - **Link**: [Paper](https://arxiv.org/abs/1707.06347)

### 14. **Vision Transformers (ViT)**
   - **Paper**: *An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale* (2020) by Dosovitskiy et al.  
   - **Link**: [Paper](https://arxiv.org/abs/2010.11929)

### 15. **Diffusion Models**
   - **Paper**: *Denoising Diffusion Probabilistic Models* (2020) by Ho et al.  
   - **Link**: [Paper](https://arxiv.org/abs/2006.11239)


## Usage


## License

This repository is licensed under the Creative Commons Attribution-NonCommercial 4.0 International License (CC BY-NC 4.0) - see the [LICENSE](LICENSE) file for details.